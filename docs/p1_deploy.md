# Service Containerization (Implementation & Deployment Decisions)

Some of these decisions depend on the chosen nice-to-have categories, but the current plan already covers the must-have scope below.

## Chosen Approach to Containerize Must-Have Services

All seven must-have services (frontend, user, question, matching, history, collaboration, chat) keep a dedicated Dockerfile and standalone `docker-compose.yml`. The root-level `docker-compose.yml` simply includes each sub-service so local developers can spin up the entire stack, while `docker-compose.prod.yml` removes the local Postgres/Redis sidecars and expects Terraform-managed RDS/ElastiCache instead. Backends use slim Python bases with an entrypoint that blocks on dependencies, runs migrations, and exposes `/health`. The frontend builds under Node 22 and serves static assets out of an Nginx stage.

```mermaid
%%{init: {'theme':'base', 'themeVariables': {'fontSize':'14px'}}}%%
graph LR
  subgraph ServiceRepos["Service Repos"]
    FE["frontend/Dockerfile"]
    US["user_service/Dockerfile"]
    QS["question_service/Dockerfile"]
    MS["matching_service/Dockerfile"]
    HS["history_service/Dockerfile"]
    CS["collaboration_service/Dockerfile"]
    CH["chat_service/Dockerfile"]
  end

  FE --> FE_IMG["Frontend image<br/>node:22-slim → nginx:alpine"]
  US --> US_IMG["User image<br/>python:3.10-slim"]
  QS --> QS_IMG["Question image<br/>python:3.10-slim"]
  MS --> MS_IMG["Matching image<br/>python:3.13-slim"]
  HS --> HS_IMG["History image<br/>python:3.10-slim"]
  CS --> CS_IMG["Collab image<br/>python:3.10-slim"]
  CH --> CH_IMG["Chat image<br/>python:3.10-slim"]

  FE_IMG --> Compose["docker-compose.yml include tree"]
  US_IMG --> Compose
  QS_IMG --> Compose
  MS_IMG --> Compose
  HS_IMG --> Compose
  CS_IMG --> Compose
  CH_IMG --> Compose
  Compose --> DevStack["Local dev stack<br/>with hot-reload volumes"]
  Compose --> ProdOverlay["docker-compose.prod.yml overrides"]
  ProdOverlay --> Terraform["ECS task definitions<br/>generated by Terraform"]
  Terraform --> Fargate["ECS Fargate services<br/>wired to RDS/ElastiCache"]

  classDef wider padding:10px
  class FE_IMG,US_IMG,QS_IMG,MS_IMG,HS_IMG,CS_IMG,CH_IMG,Compose,DevStack,ProdOverlay,Terraform,Fargate wider
```

## Deployment Workflow and CI/CD Considerations

Operators follow scripted steps today: validate prerequisites, deploy infrastructure, build and push ECR images, run migrations, verify the stack, and optionally force ECS to refresh tasks. `DEPLOY.sh` chains those scripts; the next increment is to encode the same stages inside GitHub Actions so build » test » scan » push is gated automatically before a Terraform apply and production deploy.

```mermaid
%%{init: {'theme':'base', 'themeVariables': {'fontSize':'14px'}}}%%
flowchart TD
  start([Kick off DEPLOY.sh]) --> checklist["00-pre-deployment-checklist.sh<br/>Tools & creds OK?"]
  checklist -->|pass| tf["02-deploy-infrastructure.sh<br/>terraform init/plan/apply"]
  tf --> build["01-build-and-push-images.sh<br/>docker build linux/amd64 → ECR push"]
  build --> migrate["03-run-migrations.sh<br/>Apply Django migrations"]
  migrate --> verify["04-verify-deployment.sh<br/>Smoke tests: VPC/ALB/health"]
  verify --> refresh["05-force-service-update.sh<br/>Force new ECS deployment"]
  refresh --> done([Deployment complete])
  checklist -->|fail| stop1([Fix issues, rerun])
  tf -->|plan rejected| stop2([Abort until reviewed])

  classDef wider padding:10px
  class checklist,tf,build,migrate,verify,refresh wider
```

## Alignment with Scalability and Production Readiness

Terraform provisions an ECS Fargate cluster, Cloud Map service discovery, an ALB with path-based routing, and autoscaling policies anchored on CPU, memory, and ALB request counts. Each must-have service defaults to two tasks, leaves 50% of capacity healthy during deploys (max 200%), and uses ECS’ deployment circuit breaker so failed rollouts revert automatically. Managed RDS/Redis back the stateful pieces, while compose parity ensures dev/prod behavior matches.

```mermaid
%%{init: {'theme':'base', 'themeVariables': {'fontSize':'14px'}}}%%
graph TD
  ALB["Application Load Balancer<br/>path-based routing"] --> UserTG["user-service TG"]
  ALB --> QuestionTG["question-service TG"]
  ALB --> MatchingTG["matching-service TG"]
  ALB --> HistoryTG["history-service TG"]
  ALB --> CollabTG["collaboration-service TG"]
  ALB --> ChatTG["chat-service TG"]
  ALB --> FrontendTG["frontend TG"]

  UserTG --> UserECS["ECS user-service<br/>2 tasks, circuit breaker"]
  QuestionTG --> QuestionECS["ECS question-service"]
  MatchingTG --> MatchingECS["ECS matching-service"]
  HistoryTG --> HistoryECS["ECS history-service"]
  CollabTG --> CollabECS["ECS collaboration-service"]
  ChatTG --> ChatECS["ECS chat-service"]
  FrontendTG --> FrontendECS["ECS frontend"]

  UserECS --> RDSUser["RDS PostgreSQL<br/>user_db"]
  QuestionECS --> RDSQuestion["RDS PostgreSQL<br/>question_db"]
  MatchingECS --> RDSMatching["RDS PostgreSQL<br/>matching_db"]
  HistoryECS --> RDSHistory["RDS PostgreSQL<br/>history_db"]
  MatchingECS --> RedisMatching["ElastiCache Redis<br/>matching"]
  CollabECS --> RedisCollab["ElastiCache Redis<br/>collaboration"]
  ChatECS --> RedisChat["ElastiCache Redis<br/>chat"]

  subgraph AutoscalingPolicies["Autoscaling"]
    CPU["CPU target"]
    MEM["Memory target"]
    REQ["ALB request/target"]
  end
  AutoscalingPolicies --> UserECS
  AutoscalingPolicies --> QuestionECS
  AutoscalingPolicies --> MatchingECS
  AutoscalingPolicies --> HistoryECS
  AutoscalingPolicies --> CollabECS
  AutoscalingPolicies --> ChatECS
  AutoscalingPolicies --> FrontendECS

  CloudMap["Cloud Map DNS<br/>*.peerprep-prod.local"] --> UserECS
  CloudMap --> QuestionECS
  CloudMap --> MatchingECS
  CloudMap --> HistoryECS
  CloudMap --> CollabECS
  CloudMap --> ChatECS
  CloudMap --> FrontendECS

  classDef wider padding:10px
  class ALB,UserTG,QuestionTG,MatchingTG,HistoryTG,CollabTG,ChatTG,FrontendTG,UserECS,QuestionECS,MatchingECS,HistoryECS,CollabECS,ChatECS,FrontendECS,RDSUser,RDSQuestion,RDSMatching,RDSHistory,RedisMatching,RedisCollab,RedisChat,CloudMap wider
```

## Implementation Tech Stack

Backends run Django (user/question/history) or FastAPI/Socket.IO (matching/collaboration/chat) on slim Python images and install dependencies via `pip`; the frontend builds with Vite under Node 22, then serves static assets through Nginx with runtime `envsubst`. `01-build-and-push-images.sh` enforces `--platform linux/amd64`, clears caches, and tags both local and ECR images. Image/security scanning is currently manual—adding Trivy or ECR scans plus Git SHA tags is the next increment.

```mermaid
%%{init: {'theme':'base', 'themeVariables': {'fontSize':'14px'}}}%%
graph LR
  subgraph FrontendStack["Frontend"]
    FEStack["Vite + React<br/>node:22-slim → nginx:alpine"]
    npmci["npm ci"]
  end

  subgraph DjangoServices["Django Services"]
    UserSvc["user_service<br/>python:3.10-slim"]
    QuestionSvc["question_service<br/>python:3.10-slim"]
    HistorySvc["history_service<br/>python:3.10-slim"]
    DjangoEntrypoint["migrate + collectstatic<br/>entrypoint"]
  end

  subgraph FastAPISocket["FastAPI & Socket"]
    MatchingSvc["matching_service<br/>python:3.13-slim"]
    CollabSvc["collaboration_service<br/>python:3.10-slim"]
    ChatSvc["chat_service<br/>python:3.10-slim"]
    FastAPIEntrypoint["uvicorn/socket.io start"]
  end

  npmci --> FEStack --> ImageBuild["Multi-stage build<br/>cache clean"]
  UserSvc --> DjangoEntrypoint
  QuestionSvc --> DjangoEntrypoint
  HistorySvc --> DjangoEntrypoint
  DjangoEntrypoint --> ImageBuild
  MatchingSvc --> FastAPIEntrypoint
  CollabSvc --> FastAPIEntrypoint
  ChatSvc --> FastAPIEntrypoint
  FastAPIEntrypoint --> ImageBuild
  ImageBuild --> ECRTag["ECR tag :latest<br/>+ planned Git SHA"]
  ECRTag -. future .-> Trivy["Security scan gate"]

  classDef wider padding:10px
  class FEStack,npmci,UserSvc,QuestionSvc,HistorySvc,DjangoEntrypoint,MatchingSvc,CollabSvc,ChatSvc,FastAPIEntrypoint,ImageBuild,ECRTag,Trivy wider
```

## Configuration and Secrets

Local developers copy `.env.sample`/`.env.prod.sample` into `.env`/`.env.prod`, which docker-compose reads. Terraform consumes `terraform.tfvars` (holding non-committed values like `db_password`, `secret_key`) and injects them into ECS task definitions as environment variables. `ENVIRONMENT_VARIABLES.md` documents the full matrix. Secrets live in Terraform variables for now; the ECS execution role already has `secretsmanager:GetSecretValue`, so we can later migrate to AWS Secrets Manager/SSM via the module’s `secrets` field.

```mermaid
%%{init: {'theme':'base', 'themeVariables': {'fontSize':'14px'}}}%%
graph TD
  Samples[".env.sample<br/>.env.prod.sample"] --> LocalEnv[".env / .env.prod"]
  LocalEnv --> ComposeEnv["docker-compose dev/prod"]

  TFVars["terraform.tfvars<br/>db_password, secret_key"] --> TerraformApply["terraform apply"]
  TerraformApply --> TaskDefs["ECS task definitions<br/>environment variables"]
  TaskDefs --> FargateTasks["Fargate tasks"]

  FargateTasks --> Services["Running services"]
  Services --> Usage["Load env vars & service discovery"]

  SecretsMgr["AWS Secrets Manager / SSM"] -. planned migration .-> TaskDefs

  classDef wider padding:10px
  class Samples,LocalEnv,ComposeEnv,TFVars,TerraformApply,TaskDefs,FargateTasks,Services,Usage,SecretsMgr wider
```

## Networking and Ingress

Compose attaches every container to a shared bridge network plus service-specific networks for their databases/Redis (only necessary ports exposed). In AWS, Terraform builds VPC subnets, an ALB with path-based listener rules, and Cloud Map DNS so backend services call each other via `<service>.peerprep-prod.local`. The frontend Nginx template proxies the `/something-service-api` paths to the internal hostnames, and WebSocket workloads use ALB cookie stickiness.

```mermaid
%%{init: {'theme':'base', 'themeVariables': {'fontSize':'14px'}}}%%
graph LR
  subgraph LocalCompose["Local Compose"]
    FrontendC["frontend :80/:5173"] --> SharedNet["shared_network"]
    UserC["user-service :8001→8000"] --> SharedNet
    QuestionC["question-service :8002→8000"] --> SharedNet
    MatchingC["matching-service :8003→8000"] --> SharedNet
    HistoryC["history-service :8004→8000"] --> SharedNet
    CollabC["collaboration-service :8005→8000"] --> SharedNet
    ChatC["chat-service :8006→8000"] --> SharedNet
  end

  subgraph AWSVPC["AWS VPC"]
    ALB2["ALB<br/>public subnets"] -->|"/user-service-api/*"| UserTG
    ALB2 -->|"/question-service-api/*"| QuestionTG
    ALB2 -->|"/matching-service-api/*"| MatchingTG
    ALB2 -->|"/history-service-api/*"| HistoryTG
    ALB2 -->|"/collaboration-service-api/*"| CollabTG
    ALB2 -->|"/chat-service-api/*"| ChatTG
    ALB2 -->|"/"| FrontendTG

    UserTG --> UserTask["ECS user-service<br/>private subnet"]
    QuestionTG --> QuestionTask["ECS question-service"]
    MatchingTG --> MatchingTask["ECS matching-service"]
    HistoryTG --> HistoryTask["ECS history-service"]
    CollabTG --> CollabTask["ECS collaboration-service<br/>ALB cookie stickiness"]
    ChatTG --> ChatTask["ECS chat-service<br/>ALB cookie stickiness"]
    FrontendTG --> FrontendTask["ECS frontend"]
  end

  CloudMap["Cloud Map DNS<br/>service.peerprep-prod.local"] --> UserTask
  CloudMap --> QuestionTask
  CloudMap --> MatchingTask
  CloudMap --> HistoryTask
  CloudMap --> CollabTask
  CloudMap --> ChatTask
  CloudMap --> FrontendTask

  classDef wider padding:10px
  class FrontendC,UserC,QuestionC,MatchingC,HistoryC,CollabC,ChatC,SharedNet,ALB2,UserTG,QuestionTG,MatchingTG,HistoryTG,CollabTG,ChatTG,FrontendTG,UserTask,QuestionTask,MatchingTask,HistoryTask,CollabTask,ChatTask,FrontendTask,CloudMap wider
```

## CI/CD and Rollout Strategy

The documented flow covers build → migrate → verify. When scripts run manually, Terraform plan/apply pauses for approval, then ECS performs rolling updates (min healthy 50%, max 200%) with the circuit breaker enabled. `05-force-service-update.sh` can trigger a new deployment after fresh images land. Automating the same sequence inside CI/CD will let us gate dev → staging → prod promotions behind automated tests and change approvals.

```mermaid
%%{init: {'theme':'base', 'themeVariables': {'fontSize':'14px'}}}%%
stateDiagram-v2
  [*] --> DevBranch
  DevBranch --> CI_Build: PR push
  CI_Build --> CI_Test
  CI_Test --> CI_Scan
  CI_Scan --> MainMerge: Merge to main
  MainMerge --> ECR_Push: Build and push images
  ECR_Push --> TerraformPlan: Manual approval
  TerraformPlan --> TerraformApply
  TerraformApply --> Migrations
  Migrations --> ECSRolling
  ECSRolling --> SmokeTests: Success
  SmokeTests --> ProdLive
  ProdLive --> [*]
  ECSRolling --> Rollback: Failure detected
  Rollback --> ProdLive

  note right of ECSRolling
    ECS rolling deploy
    minHealthy=50%
    max=200%
    circuit breaker enabled
  end note

  note right of SmokeTests
    04-verify-deployment.sh
  end note
```

## Observability, Health Checks, and Alerts

`ADD_HEALTH_ENDPOINTS.md` describes the `/health` endpoints every service should expose. Those endpoints power docker-compose health checks, ALB target group probes, and ECS task health commands. Logs flow into CloudWatch (`/ecs/peerprep-prod`) with Container Insights enabled. `04-verify-deployment.sh` performs post-deploy smoke tests; operators can tail logs or curl `/health` through the ALB. Next steps are to harden automated health dashboards and alerting on CPU/memory/request metrics, and layer in structured logging or tracing once service-level objectives are defined.

```mermaid
%%{init: {'theme':'base', 'themeVariables': {'fontSize':'14px'}}}%%
graph LR
  HealthEP["/health endpoints"] --> ComposeHC["docker-compose health checks"]
  HealthEP --> ALBHC["ALB target health"]
  HealthEP --> ECSHC["ECS task health command"]

  ECSHC --> ECS["ECS services"]
  ECS --> Logs["CloudWatch Logs<br/>/ecs/peerprep-prod"]
  ECS --> Metrics["CloudWatch metrics<br/>CPU/Mem/Request"]
  Metrics -. roadmap .-> Alerts["Dashboards & alarms"]

  VerifyScript["04-verify-deployment.sh"] --> ALBHC
  VerifyScript --> Metrics
  VerifyScript --> Logs

  Operator["Operator"] -->|"aws logs tail"| Logs
  Operator -->|"curl /health"| HealthEP

  classDef wider padding:10px
  class HealthEP,ComposeHC,ALBHC,ECSHC,ECS,Logs,Metrics,Alerts,VerifyScript,Operator wider
```
